{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d163319e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Phase 1: Data Exploration & Cleaning\\n\",\n",
    "    \"## Amazon Delivery Time Prediction Project\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook covers:\\n\",\n",
    "    \"1. Data Loading and Initial Inspection\\n\",\n",
    "    \"2. Data Quality Assessment\\n\",\n",
    "    \"3. Data Cleaning and Preprocessing\\n\",\n",
    "    \"4. Exploratory Data Analysis (EDA)\\n\",\n",
    "    \"5. Outlier Detection and Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Import required libraries\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"import warnings\\n\",\n",
    "    \"warnings.filterwarnings('ignore')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Set display options\\n\",\n",
    "    \"pd.set_option('display.max_columns', None)\\n\",\n",
    "    \"pd.set_option('display.max_rows', 100)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Set plotting style\\n\",\n",
    "    \"sns.set_style('whitegrid')\\n\",\n",
    "    \"plt.rcParams['figure.figsize'] = (12, 6)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print('✓ Libraries imported successfully!')\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Data Loading and Initial Inspection\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Load the dataset\\n\",\n",
    "    \"df = pd.read_csv('../data/raw/amazon_delivery.csv')\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Dataset loaded successfully!\\\")\\n\",\n",
    "    \"print(f\\\"Shape: {df.shape}\\\")\\n\",\n",
    "    \"print(f\\\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Display first few rows\\n\",\n",
    "    \"print(\\\"First 5 rows of the dataset:\\\")\\n\",\n",
    "    \"df.head()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Display dataset info\\n\",\n",
    "    \"print(\\\"Dataset Information:\\\")\\n\",\n",
    "    \"df.info()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Display statistical summary\\n\",\n",
    "    \"print(\\\"Statistical Summary:\\\")\\n\",\n",
    "    \"df.describe()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Data Quality Assessment\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Check for missing values\\n\",\n",
    "    \"print(\\\"Missing Values Analysis:\\\")\\n\",\n",
    "    \"missing_data = pd.DataFrame({\\n\",\n",
    "    \"    'Column': df.columns,\\n\",\n",
    "    \"    'Missing_Count': df.isnull().sum().values,\\n\",\n",
    "    \"    'Missing_Percentage': (df.isnull().sum().values / len(df) * 100).round(2)\\n\",\n",
    "    \"})\\n\",\n",
    "    \"\\n\",\n",
    "    \"missing_data = missing_data[missing_data['Missing_Count'] > 0].sort_values(\\n\",\n",
    "    \"    'Missing_Percentage', ascending=False\\n\",\n",
    "    \").reset_index(drop=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"if not missing_data.empty:\\n\",\n",
    "    \"    print(missing_data)\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"✓ No missing values found!\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Check for duplicate rows\\n\",\n",
    "    \"duplicate_count = df.duplicated().sum()\\n\",\n",
    "    \"duplicate_pct = (duplicate_count / len(df)) * 100\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Duplicate Rows Analysis:\\\")\\n\",\n",
    "    \"print(f\\\"  Total duplicates: {duplicate_count} ({duplicate_pct:.2f}%)\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"if duplicate_count > 0:\\n\",\n",
    "    \"    print(f\\\"  ⚠️ Found {duplicate_count} duplicate rows\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"  ✓ No duplicate rows found\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Check unique values for categorical columns\\n\",\n",
    "    \"print(\\\"Categorical Columns Analysis:\\\")\\n\",\n",
    "    \"categorical_cols = df.select_dtypes(include=['object']).columns\\n\",\n",
    "    \"\\n\",\n",
    "    \"for col in categorical_cols:\\n\",\n",
    "    \"    unique_count = df[col].nunique()\\n\",\n",
    "    \"    print(f\\\"\\\\n{col}:\\\")\\n\",\n",
    "    \"    print(f\\\"  Unique values: {unique_count}\\\")\\n\",\n",
    "    \"    print(f\\\"  Values: {df[col].unique()[:10].tolist()}\\\")\\n\",\n",
    "    \"    if unique_count > 10:\\n\",\n",
    "    \"        print(f\\\"  ... and {unique_count - 10} more\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. Data Cleaning\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Create a copy for cleaning\\n\",\n",
    "    \"df_clean = df.copy()\\n\",\n",
    "    \"print(f\\\"Original shape: {df.shape}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Step 1: Remove whitespace from string columns\\n\",\n",
    "    \"print(\\\"Step 1: Removing whitespace...\\\")\\n\",\n",
    "    \"for col in df_clean.select_dtypes(include=['object']).columns:\\n\",\n",
    "    \"    df_clean[col] = df_clean[col].str.strip()\\n\",\n",
    "    \"print(\\\"✓ Whitespace removed\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Step 2: Standardize categorical values\\n\",\n",
    "    \"print(\\\"Step 2: Standardizing categorical values...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Fix Area column spelling\\n\",\n",
    "    \"df_clean['Area'] = df_clean['Area'].replace({\\n\",\n",
    "    \"    'Metropolitian': 'Metropolitan'\\n\",\n",
    "    \"})\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Check unique values after standardization\\n\",\n",
    "    \"print(\\\"\\\\nUnique values after standardization:\\\")\\n\",\n",
    "    \"for col in ['Weather', 'Traffic', 'Vehicle', 'Area']:\\n\",\n",
    "    \"    print(f\\\"{col}: {df_clean[col].unique().tolist()}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"✓ Categories standardized\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Step 3: Remove duplicates\\n\",\n",
    "    \"print(\\\"Step 3: Removing duplicates...\\\")\\n\",\n",
    "    \"rows_before = len(df_clean)\\n\",\n",
    "    \"df_clean.drop_duplicates(inplace=True)\\n\",\n",
    "    \"rows_after = len(df_clean)\\n\",\n",
    "    \"print(f\\\"✓ Removed {rows_before - rows_after} duplicate rows\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Step 4: Handle missing values\\n\",\n",
    "    \"print(\\\"Step 4: Handling missing values...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# For numerical columns: use median\\n\",\n",
    "    \"numerical_cols = df_clean.select_dtypes(include=[np.number]).columns\\n\",\n",
    "    \"for col in numerical_cols:\\n\",\n",
    "    \"    if df_clean[col].isnull().sum() > 0:\\n\",\n",
    "    \"        df_clean[col].fillna(df_clean[col].median(), inplace=True)\\n\",\n",
    "    \"        print(f\\\"  ✓ Filled {col} with median\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# For categorical columns: use mode\\n\",\n",
    "    \"categorical_cols = df_clean.select_dtypes(include=['object']).columns\\n\",\n",
    "    \"for col in categorical_cols:\\n\",\n",
    "    \"    if df_clean[col].isnull().sum() > 0:\\n\",\n",
    "    \"        mode_value = df_clean[col].mode()[0] if not df_clean[col].mode().empty else 'Unknown'\\n\",\n",
    "    \"        df_clean[col].fillna(mode_value, inplace=True)\\n\",\n",
    "    \"        print(f\\\"  ✓ Filled {col} with mode\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nMissing values after cleaning: {df_clean.isnull().sum().sum()}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Step 5: Convert data types\\n\",\n",
    "    \"print(\\\"Step 5: Converting data types...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Convert Order_Date to datetime\\n\",\n",
    "    \"df_clean['Order_Date'] = pd.to_datetime(df_clean['Order_Date'])\\n\",\n",
    "    \"print(\\\"✓ Converted Order_Date to datetime\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Convert time columns\\n\",\n",
    "    \"df_clean['Order_Time'] = pd.to_datetime(df_clean['Order_Time'], format='%H:%M:%S').dt.time\\n\",\n",
    "    \"df_clean['Pickup_Time'] = pd.to_datetime(df_clean['Pickup_Time'], format='%H:%M:%S').dt.time\\n\",\n",
    "    \"print(\\\"✓ Converted time columns\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nCleaned dataset shape: {df_clean.shape}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 4. Exploratory Data Analysis (EDA)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 4.1 Target Variable Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Delivery Time distribution\\n\",\n",
    "    \"fig, axes = plt.subplots(1, 3, figsize=(18, 5))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Histogram\\n\",\n",
    "    \"axes[0].hist(df_clean['Delivery_Time'], bins=50, edgecolor='black', alpha=0.7)\\n\",\n",
    "    \"axes[0].set_title('Delivery Time Distribution', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"axes[0].set_xlabel('Delivery Time (minutes)')\\n\",\n",
    "    \"axes[0].set_ylabel('Frequency')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Box plot\\n\",\n",
    "    \"axes[1].boxplot(df_clean['Delivery_Time'])\\n\",\n",
    "    \"axes[1].set_title('Delivery Time Box Plot', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"axes[1].set_ylabel('Delivery Time (minutes)')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Q-Q plot\\n\",\n",
    "    \"from scipy import stats\\n\",\n",
    "    \"stats.probplot(df_clean['Delivery_Time'], dist=\\\"norm\\\", plot=axes[2])\\n\",\n",
    "    \"axes[2].set_title('Q-Q Plot', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Statistics\\n\",\n",
    "    \"print(\\\"\\\\nDelivery Time Statistics:\\\")\\n\",\n",
    "    \"print(df_clean['Delivery_Time'].describe())\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 4.2 Numerical Features Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Distribution of numerical features\\n\",\n",
    "    \"numerical_features = ['Agent_Age', 'Agent_Rating', 'Delivery_Time']\\n\",\n",
    "    \"\\n\",\n",
    "    \"fig, axes = plt.subplots(1, 3, figsize=(18, 5))\\n\",\n",
    "    \"\\n\",\n",
    "    \"for idx, col in enumerate(numerical_features):\\n\",\n",
    "    \"    axes[idx].hist(df_clean[col], bins=30, edgecolor='black', alpha=0.7, color='steelblue')\\n\",\n",
    "    \"    axes[idx].set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\\n\",\n",
    "    \"    axes[idx].set_xlabel(col)\\n\",\n",
    "    \"    axes[idx].set_ylabel('Frequency')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Add mean and median lines\\n\",\n",
    "    \"    mean_val = df_clean[col].mean()\\n\",\n",
    "    \"    median_val = df_clean[col].median()\\n\",\n",
    "    \"    axes[idx].axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.2f}')\\n\",\n",
    "    \"    axes[idx].axvline(median_val, color='green', linestyle='--', linewidth=2, label=f'Median: {median_val:.2f}')\\n\",\n",
    "    \"    axes[idx].legend()\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 4.3 Categorical Features Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Distribution of categorical features\\n\",\n",
    "    \"categorical_features = ['Weather', 'Traffic', 'Vehicle', 'Area']\\n\",\n",
    "    \"\\n\",\n",
    "    \"fig, axes = plt.subplots(2, 2, figsize=(16, 12))\\n\",\n",
    "    \"axes = axes.flatten()\\n\",\n",
    "    \"\\n\",\n",
    "    \"for idx, col in enumerate(categorical_features):\\n\",\n",
    "    \"    value_counts = df_clean[col].value_counts()\\n\",\n",
    "    \"    axes[idx].bar(range(len(value_counts)), value_counts.values, \\n\",\n",
    "    \"                  color='coral', edgecolor='black')\\n\",\n",
    "    \"    axes[idx].set_xticks(range(len(value_counts)))\\n\",\n",
    "    \"    axes[idx].set_xticklabels(value_counts.index, rotation=45, ha='right')\\n\",\n",
    "    \"    axes[idx].set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\\n\",\n",
    "    \"    axes[idx].set_xlabel(col)\\n\",\n",
    "    \"    axes[idx].set_ylabel('Count')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Add count labels\\n\",\n",
    "    \"    for i, v in enumerate(value_counts.values):\\n\",\n",
    "    \"        axes[idx].text(i, v + max(value_counts.values)*0.01, str(v), \\n\",\n",
    "    \"                      ha='center', va='bottom', fontweight='bold')\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 4.4 Bivariate Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Delivery Time by categorical features\\n\",\n",
    "    \"fig, axes = plt.subplots(2, 2, figsize=(16, 12))\\n\",\n",
    "    \"axes = axes.flatten()\\n\",\n",
    "    \"\\n\",\n",
    "    \"for idx, col in enumerate(categorical_features):\\n\",\n",
    "    \"    df_clean.boxplot(column='Delivery_Time', by=col, ax=axes[idx])\\n\",\n",
    "    \"    axes[idx].set_title(f'Delivery Time by {col}', fontsize=12, fontweight='bold')\\n\",\n",
    "    \"    axes[idx].set_xlabel(col)\\n\",\n",
    "    \"    axes[idx].set_ylabel('Delivery Time (minutes)')\\n\",\n",
    "    \"    plt.suptitle('')\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Print statistics\n",
    "print(\"\\\\nAverage Delivery Time by Category:\\\")\\n\",\n",
    "    \"for col in categorical_features:\\n\",\n",
    "    \"    print(f\\\"\\\\n{col}:\\\")\\n\",\n",
    "    \"    stats_df = df_clean.groupby(col)['Delivery_Time'].agg(['mean', 'median', 'count'])\\n\",\n",
    "    \"    print(stats_df.sort_values('mean', ascending=False))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Correlation analysis\\n\",\n",
    "    \"numerical_cols = df_clean.select_dtypes(include=[np.number]).columns\\n\",\n",
    "    \"corr_matrix = df_clean[numerical_cols].corr()\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.figure(figsize=(10, 8))\\n\",\n",
    "    \"sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', \\n\",\n",
    "    \"            center=0, square=True, linewidths=1)\\n\",\n",
    "    \"plt.title('Correlation Heatmap', fontsize=16, fontweight='bold')\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Print high correlations\\n\",\n",
    "    \"print(\\\"\\\\nHigh correlations with Delivery_Time (|r| > 0.3):\\\")\\n\",\n",
    "    \"delivery_corr = corr_matrix['Delivery_Time'].sort_values(ascending=False)\\n\",\n",
    "    \"print(delivery_corr[abs(delivery_corr) > 0.3])\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 4.5 Time-Based Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Extract time features\\n\",\n",
    "    \"df_clean['Day_of_Week'] = df_clean['Order_Date'].dt.day_name()\\n\",\n",
    "    \"df_clean['Month'] = df_clean['Order_Date'].dt.month_name()\\n\",\n",
    "    \"df_clean['Hour'] = df_clean['Order_Date'].apply(lambda x: pd.to_datetime(str(x)).hour)\\n\",\n",
    "    \"\\n\",\n",
    "    \"fig, axes = plt.subplots(2, 2, figsize=(16, 12))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Orders by day of week\\n\",\n",
    "    \"day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\\n\",\n",
    "    \"day_counts = df_clean['Day_of_Week'].value_counts().reindex(day_order)\\n\",\n",
    "    \"axes[0, 0].bar(range(len(day_counts)), day_counts.values, color='skyblue', edgecolor='black')\\n\",\n",
    "    \"axes[0, 0].set_xticks(range(len(day_counts)))\\n\",\n",
    "    \"axes[0, 0].set_xticklabels(day_order, rotation=45, ha='right')\\n\",\n",
    "    \"axes[0, 0].set_title('Orders by Day of Week', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"axes[0, 0].set_ylabel('Count')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Average delivery time by day of week\\n\",\n",
    "    \"day_delivery = df_clean.groupby('Day_of_Week')['Delivery_Time'].mean().reindex(day_order)\\n\",\n",
    "    \"axes[0, 1].bar(range(len(day_delivery)), day_delivery.values, color='coral', edgecolor='black')\\n\",\n",
    "    \"axes[0, 1].set_xticks(range(len(day_delivery)))\\n\",\n",
    "    \"axes[0, 1].set_xticklabels(day_order, rotation=45, ha='right')\\n\",\n",
    "    \"axes[0, 1].set_title('Avg Delivery Time by Day', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"axes[0, 1].set_ylabel('Avg Delivery Time (min)')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Orders over time\\n\",\n",
    "    \"daily_orders = df_clean.groupby('Order_Date').size()\\n\",\n",
    "    \"axes[1, 0].plot(daily_orders.index, daily_orders.values, color='green', linewidth=2)\\n\",\n",
    "    \"axes[1, 0].set_title('Orders Over Time', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"axes[1, 0].set_xlabel('Date')\\n\",\n",
    "    \"axes[1, 0].set_ylabel('Number of Orders')\\n\",\n",
    "    \"axes[1, 0].tick_params(axis='x', rotation=45)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Product category distribution\\n\",\n",
    "    \"category_counts = df_clean['Category'].value_counts().head(10)\\n\",\n",
    "    \"axes[1, 1].barh(range(len(category_counts)), category_counts.values, color='purple')\\n\",\n",
    "    \"axes[1, 1].set_yticks(range(len(category_counts)))\\n\",\n",
    "    \"axes[1, 1].set_yticklabels(category_counts.index)\\n\",\n",
    "    \"axes[1, 1].set_title('Top 10 Product Categories', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"axes[1, 1].set_xlabel('Count')\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 5. Outlier Detection and Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Function to detect outliers using IQR\\n\",\n",
    "    \"def detect_outliers_iqr(data, column, threshold=1.5):\\n\",\n",
    "    \"    Q1 = data[column].quantile(0.25)\\n\",\n",
    "    \"    Q3 = data[column].quantile(0.75)\\n\",\n",
    "    \"    IQR = Q3 - Q1\\n\",\n",
    "    \"    lower_bound = Q1 - threshold * IQR\\n\",\n",
    "    \"    upper_bound = Q3 + threshold * IQR\\n\",\n",
    "    \"    outliers = (data[column] < lower_bound) | (data[column] > upper_bound)\\n\",\n",
    "    \"    return outliers, lower_bound, upper_bound\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Analyze outliers for key numerical columns\\n\",\n",
    "    \"outlier_cols = ['Agent_Age', 'Agent_Rating', 'Delivery_Time']\\n\",\n",
    "    \"\\n\",\n",
    "    \"fig, axes = plt.subplots(len(outlier_cols), 2, figsize=(16, 12))\\n\",\n",
    "    \"\\n\",\n",
    "    \"for idx, col in enumerate(outlier_cols):\\n\",\n",
    "    \"    outliers, lower, upper = detect_outliers_iqr(df_clean, col)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Box plot\\n\",\n",
    "    \"    axes[idx, 0].boxplot(df_clean[col])\\n\",\n",
    "    \"    axes[idx, 0].axhline(y=lower, color='r', linestyle='--', label=f'Lower: {lower:.2f}')\\n\",\n",
    "    \"    axes[idx, 0].axhline(y=upper, color='r', linestyle='--', label=f'Upper: {upper:.2f}')\\n\",\n",
    "    \"    axes[idx, 0].set_title(f'Box Plot: {col}', fontsize=12, fontweight='bold')\\n\",\n",
    "    \"    axes[idx, 0].set_ylabel(col)\\n\",\n",
    "    \"    axes[idx, 0].legend()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Histogram with outliers highlighted\\n\",\n",
    "    \"    axes[idx, 1].hist(df_clean[col], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\\n\",\n",
    "    \"    if outliers.sum() > 0:\\n\",\n",
    "    \"        axes[idx, 1].hist(df_clean[outliers][col], bins=20, alpha=0.7, \\n\",\n",
    "    \"                         color='red', edgecolor='black', label='Outliers')\\n\",\n",
    "    \"    axes[idx, 1].set_title(f'Distribution: {col}', fontsize=12, fontweight='bold')\\n\",\n",
    "    \"    axes[idx, 1].set_xlabel(col)\\n\",\n",
    "    \"    axes[idx, 1].set_ylabel('Frequency')\\n\",\n",
    "    \"    axes[idx, 1].legend()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"\\\\n{col}:\\\")\\n\",\n",
    "    \"    print(f\\\"  Outliers: {outliers.sum()} ({outliers.sum()/len(df_clean)*100:.2f}%)\\\")\\n\",\n",
    "    \"    print(f\\\"  Bounds: [{lower:.2f}, {upper:.2f}]\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 6. Key Insights Summary\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(\\\"=\\\"*70)\\n\",\n",
    "    \"print(\\\"KEY INSIGHTS FROM PHASE 1 ANALYSIS\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*70)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\n1. DATASET OVERVIEW:\\\")\\n\",\n",
    "    \"print(f\\\"   • Total records: {len(df_clean):,}\\\")\\n\",\n",
    "    \"print(f\\\"   • Features: {len(df_clean.columns)}\\\")\\n\",\n",
    "    \"print(f\\\"   • Date range: {df_clean['Order_Date'].min()} to {df_clean['Order_Date'].max()}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\n2. DELIVERY TIME INSIGHTS:\\\")\\n\",\n",
    "    \"print(f\\\"   • Average: {df_clean['Delivery_Time'].mean():.2f} minutes\\\")\\n\",\n",
    "    \"print(f\\\"   • Median: {df_clean['Delivery_Time'].median():.2f} minutes\\\")\\n\",\n",
    "    \"print(f\\\"   • Range: {df_clean['Delivery_Time'].min():.0f} - {df_clean['Delivery_Time'].max():.0f} minutes\\\")\\n\",\n",
    "    \"print(f\\\"   • Std Dev: {df_clean['Delivery_Time'].std():.2f} minutes\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\n3. WEATHER IMPACT:\\\")\\n\",\n",
    "    \"weather_impact = df_clean.groupby('Weather')['Delivery_Time'].mean().sort_values(ascending=False)\\n\",\n",
    "    \"for weather, time in weather_impact.items():\\n\",\n",
    "    \"    print(f\\\"   • {weather}: {time:.2f} min\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\n4. TRAFFIC IMPACT:\\\")\\n\",\n",
    "    \"traffic_impact = df_clean.groupby('Traffic')['Delivery_Time'].mean().sort_values(ascending=False)\\n\",\n",
    "    \"for traffic, time in traffic_impact.items():\\n\",\n",
    "    \"    print(f\\\"   • {traffic}: {time:.2f} min\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\n5. VEHICLE EFFICIENCY:\\\")\\n\",\n",
    "    \"vehicle_impact = df_clean.groupby('Vehicle')['Delivery_Time'].mean().sort_values()\\n\",\n",
    "    \"for vehicle, time in vehicle_impact.items():\\n\",\n",
    "    \"    print(f\\\"   • {vehicle}: {time:.2f} min\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\n6. AREA ANALYSIS:\\\")\\n\",\n",
    "    \"area_impact = df_clean.groupby('Area')['Delivery_Time'].mean().sort_values(ascending=False)\\n\",\n",
    "    \"for area, time in area_impact.items():\\n\",\n",
    "    \"    print(f\\\"   • {area}: {time:.2f} min\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\n7. DATA QUALITY:\\\")\\n\",\n",
    "    \"print(f\\\"   • Missing values: {df_clean.isnull().sum().sum()}\\\")\\n\",\n",
    "    \"print(f\\\"   • Duplicate rows: {df_clean.duplicated().sum()}\\\")\\n\",\n",
    "    \"print(f\\\"   • Data types: All converted appropriately\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*70)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 7. Save Cleaned Data\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Save cleaned dataset\\n\",\n",
    "    \"output_path = '../data/processed/cleaned_data.csv'\\n\",\n",
    "    \"df_clean.to_csv(output_path, index=False)\\n\",\n",
    "    \"print(f\\\"✅ Cleaned data saved to: {output_path}\\\")\\n\",\n",
    "    \"print(f\\\"\\\\nCleaned dataset shape: {df_clean.shape}\\\")\\n\",\n",
    "    \"print(f\\\"Records removed: {len(df) - len(df_clean)}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 8. Next Steps\\n\",\n",
    "    \"\\n\",\n",
    "    \"Based on the analysis, the following steps are recommended for **Phase 2: Feature Engineering**:\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Feature Engineering Tasks:\\n\",\n",
    "    \"1. **Geospatial Features**:\\n\",\n",
    "    \"   - Calculate distance between store and drop locations using Haversine formula\\n\",\n",
    "    \"   - Consider creating distance bins (short, medium, long)\\n\",\n",
    "    \"\\n\",\n",
    "    \"2. **Time-Based Features**:\\n\",\n",
    "    \"   - Extract hour of day, day of week, is_weekend\\n\",\n",
    "    \"   - Calculate time difference between order and pickup\\n\",\n",
    "    \"   - Create time_of_day categories (morning, afternoon, evening, night)\\n\",\n",
    "    \"\\n\",\n",
    "    \"3. **Interaction Features**:\\n\",\n",
    "    \"   - Traffic × Distance\\n\",\n",
    "    \"   - Weather × Vehicle type\\n\",\n",
    "    \"   - Area × Time of day\\n\",\n",
    "    \"\\n\",\n",
    "    \"4. **Encoding Strategies**:\\n\",\n",
    "    \"   - One-Hot Encoding for: Vehicle, Area (low cardinality)\\n\",\n",
    "    \"   - Target Encoding for: Category (high cardinality)\\n\",\n",
    "    \"   - Label Encoding for: Weather, Traffic (ordinal nature)\\n\",\n",
    "    \"\\n\",\n",
    "    \"5. **Feature Scaling**:\\n\",\n",
    "    \"   - StandardScaler for distance features\\n\",\n",
    "    \"   - MinMaxScaler for Agent_Age and Agent_Rating\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Key Observations for Modeling:\\n\",\n",
    "    \"- **Strong predictors**: Traffic, Weather, Vehicle type, Distance\\n\",\n",
    "    \"- **Target variable**: Slightly right-skewed, may benefit from log transformation\\n\",\n",
    "    \"- **Outliers**: Present but capped to reasonable ranges\\n\",\n",
    "    \"- **Missing values**: All handled appropriately\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Recommended Models for Phase 5:\\n\",\n",
    "    \"1. **Baseline**: Linear Regression\\n\",\n",
    "    \"2. **Tree-based**: Random Forest, Gradient Boosting\\n\",\n",
    "    \"3. **Advanced**: XGBoost, LightGBM\\n\",\n",
    "    \"4. **Ensemble**: Stacking/Voting Regressor\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.9.0\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
